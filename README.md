# Scratch Implementation of Feed-Forward Neural Network (FFNN)

- Built a 3-layer neural network from scratch (without ML libraries) for MNIST classification, achieving 98% test accuracy with ReLU activation and Adam optimizer.

- Implemented key components: forward/backpropagation, activation functions (ReLU/Sigmoid/Tanh), and regularization techniques (dropout, L2, early stopping).

- Extended to binary classification tasks, demonstrating FFNN's superiority over logistic regression (95% vs 60% accuracy) on non-linear data.
